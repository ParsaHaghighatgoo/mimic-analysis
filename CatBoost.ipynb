{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T06:23:59.132329Z",
     "start_time": "2024-11-05T06:23:57.142551Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from holoviews.plotting.bokeh.styles import alpha\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "import catboost as cat\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:23:59.187192Z",
     "start_time": "2024-11-05T06:23:59.133333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#importing CSV File\n",
    "df = pd.read_csv('extractedMimic.csv')"
   ],
   "id": "dcc03326e7a9f66d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:23:59.207775Z",
     "start_time": "2024-11-05T06:23:59.188197Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "23833c8144016857",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   icustay_id  hadm_id              intime             outtime    dbsource  \\\n",
       "0      205941   156324  28/5/2157 14:26:21  30/5/2157 14:18:24  metavision   \n",
       "1      252848   163315  29/7/2196 02:26:17  29/7/2196 12:02:39  metavision   \n",
       "2      237901   180937  14/2/2145 17:55:07  23/2/2145 12:43:43  metavision   \n",
       "3      207491   143962  11/6/2159 12:47:02  14/6/2159 16:31:30  metavision   \n",
       "4      293063   118489   1/1/2135 17:28:33   2/1/2135 06:56:56  metavision   \n",
       "\n",
       "  suspected_infection_time_poe  suspected_infection_time_poe_days  \\\n",
       "0           28/5/2157 15:30:00                          -0.044201   \n",
       "1           29/7/2196 04:57:00                          -0.104664   \n",
       "2           14/2/2145 21:20:00                          -0.142280   \n",
       "3           11/6/2159 12:11:00                           0.025023   \n",
       "4            1/1/2135 15:55:00                           0.064965   \n",
       "\n",
       "    specimen_poe  positiveculture_poe antibiotic_time_poe  ... glucose_min1  \\\n",
       "0    MRSA SCREEN                    0  28/5/2157 00:00:00  ...         40.0   \n",
       "1    MRSA SCREEN                    1  29/7/2196 00:00:00  ...        182.0   \n",
       "2  BLOOD CULTURE                    0  15/2/2145 00:00:00  ...        123.0   \n",
       "3  BLOOD CULTURE                    0  11/6/2159 00:00:00  ...         92.0   \n",
       "4  BLOOD CULTURE                    0   1/1/2135 00:00:00  ...        150.0   \n",
       "\n",
       "   glucose_max1  glucose_mean rrt  subject_id hadm_id.1  icustay_id.1  \\\n",
       "0         202.0     87.250000   0       88883    156324        205941   \n",
       "1         231.0    206.500000   0       46154    163315        252848   \n",
       "2         185.0    151.285714   1       42682    180937        237901   \n",
       "3         118.0    105.000000   0       45111    143962        207491   \n",
       "4         163.0    155.000000   0       56648    118489        293063   \n",
       "\n",
       "   urineoutput  colloid_bolus  crystalloid_bolus  \n",
       "0          0.0            NaN              250.0  \n",
       "1          0.0            NaN              250.0  \n",
       "2          0.0            NaN              250.0  \n",
       "3          4.0            NaN              250.0  \n",
       "4          5.0            NaN              250.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>dbsource</th>\n",
       "      <th>suspected_infection_time_poe</th>\n",
       "      <th>suspected_infection_time_poe_days</th>\n",
       "      <th>specimen_poe</th>\n",
       "      <th>positiveculture_poe</th>\n",
       "      <th>antibiotic_time_poe</th>\n",
       "      <th>...</th>\n",
       "      <th>glucose_min1</th>\n",
       "      <th>glucose_max1</th>\n",
       "      <th>glucose_mean</th>\n",
       "      <th>rrt</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id.1</th>\n",
       "      <th>icustay_id.1</th>\n",
       "      <th>urineoutput</th>\n",
       "      <th>colloid_bolus</th>\n",
       "      <th>crystalloid_bolus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205941</td>\n",
       "      <td>156324</td>\n",
       "      <td>28/5/2157 14:26:21</td>\n",
       "      <td>30/5/2157 14:18:24</td>\n",
       "      <td>metavision</td>\n",
       "      <td>28/5/2157 15:30:00</td>\n",
       "      <td>-0.044201</td>\n",
       "      <td>MRSA SCREEN</td>\n",
       "      <td>0</td>\n",
       "      <td>28/5/2157 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>87.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>88883</td>\n",
       "      <td>156324</td>\n",
       "      <td>205941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>252848</td>\n",
       "      <td>163315</td>\n",
       "      <td>29/7/2196 02:26:17</td>\n",
       "      <td>29/7/2196 12:02:39</td>\n",
       "      <td>metavision</td>\n",
       "      <td>29/7/2196 04:57:00</td>\n",
       "      <td>-0.104664</td>\n",
       "      <td>MRSA SCREEN</td>\n",
       "      <td>1</td>\n",
       "      <td>29/7/2196 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>182.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>206.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>46154</td>\n",
       "      <td>163315</td>\n",
       "      <td>252848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237901</td>\n",
       "      <td>180937</td>\n",
       "      <td>14/2/2145 17:55:07</td>\n",
       "      <td>23/2/2145 12:43:43</td>\n",
       "      <td>metavision</td>\n",
       "      <td>14/2/2145 21:20:00</td>\n",
       "      <td>-0.142280</td>\n",
       "      <td>BLOOD CULTURE</td>\n",
       "      <td>0</td>\n",
       "      <td>15/2/2145 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>151.285714</td>\n",
       "      <td>1</td>\n",
       "      <td>42682</td>\n",
       "      <td>180937</td>\n",
       "      <td>237901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207491</td>\n",
       "      <td>143962</td>\n",
       "      <td>11/6/2159 12:47:02</td>\n",
       "      <td>14/6/2159 16:31:30</td>\n",
       "      <td>metavision</td>\n",
       "      <td>11/6/2159 12:11:00</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>BLOOD CULTURE</td>\n",
       "      <td>0</td>\n",
       "      <td>11/6/2159 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>45111</td>\n",
       "      <td>143962</td>\n",
       "      <td>207491</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293063</td>\n",
       "      <td>118489</td>\n",
       "      <td>1/1/2135 17:28:33</td>\n",
       "      <td>2/1/2135 06:56:56</td>\n",
       "      <td>metavision</td>\n",
       "      <td>1/1/2135 15:55:00</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>BLOOD CULTURE</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/2135 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>56648</td>\n",
       "      <td>118489</td>\n",
       "      <td>293063</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 106 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:24:01.104343Z",
     "start_time": "2024-11-05T06:24:01.005376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.drop(df[df['age'] < 18].index, inplace=True)\n",
    "\n",
    "df.describe()\n",
    "df.info()\n"
   ],
   "id": "55ba111227e48142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4555 entries, 0 to 4558\n",
      "Columns: 106 entries, icustay_id to crystalloid_bolus\n",
      "dtypes: float64(57), int64(39), object(10)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:24:02.085562Z",
     "start_time": "2024-11-05T06:24:02.080537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df[['urineoutput', 'lactate_min','bun_mean','sysbp_min', 'metastatic_cancer', 'inr_max', 'age', 'sodium_max', 'aniongap_max', 'creatinine_min', 'spo2_mean']]\n",
    "\n",
    "y = df['thirtyday_expire_flag']"
   ],
   "id": "b64544195934e7f2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:24:03.359736Z",
     "start_time": "2024-11-05T06:24:03.353230Z"
    }
   },
   "cell_type": "code",
   "source": "X.isnull().sum()",
   "id": "2cb41592649ee3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "urineoutput            0\n",
       "lactate_min            0\n",
       "bun_mean               0\n",
       "sysbp_min              8\n",
       "metastatic_cancer      0\n",
       "inr_max              270\n",
       "age                    0\n",
       "sodium_max             0\n",
       "aniongap_max          14\n",
       "creatinine_min         2\n",
       "spo2_mean              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:24:04.260876Z",
     "start_time": "2024-11-05T06:24:04.255665Z"
    }
   },
   "cell_type": "code",
   "source": "X.shape",
   "id": "19ce73dc588a309d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4555, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:24:05.220902Z",
     "start_time": "2024-11-05T06:24:05.211844Z"
    }
   },
   "cell_type": "code",
   "source": "X.fillna(X.median(), inplace=True)",
   "id": "de11008731580418",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:24:06.675744Z",
     "start_time": "2024-11-05T06:24:06.670620Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)\n",
   "id": "964fe6d9929197bb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:37:11.073113Z",
     "start_time": "2024-11-05T06:35:39.079670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'iterations': [ 400, 800, 1000, 1500, 1800],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 5, 6, 7],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'class_weights': [[1, 2], [1, 3], [1, 4]],  # Try different class weight combinations\n",
    "    'border_count': [32, 64, 128],\n",
    "    'bagging_temperature': [0, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize the CatBoost model\n",
    "catboost_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=catboost_model,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=50,  # Increased number of iterations for better exploration\n",
    "                                   cv=3,  # 3-fold cross-validation\n",
    "                                   scoring='roc_auc',  # Optimize for AUC\n",
    "                                   random_state=42,\n",
    "                                   verbose=1,\n",
    "                                   n_jobs=-1)  # Use all processors\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from RandomizedSearchCV\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Use best_params to define a narrower grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'iterations': [best_params['iterations'] - 200, best_params['iterations'], best_params['iterations'] + 200],\n",
    "    'learning_rate': [best_params['learning_rate'] * 0.5, best_params['learning_rate'], best_params['learning_rate'] * 1.5],\n",
    "    'depth': [best_params['depth'] - 1, best_params['depth'], best_params['depth'] + 1],\n",
    "    'l2_leaf_reg': [best_params['l2_leaf_reg'] - 2, best_params['l2_leaf_reg'], best_params['l2_leaf_reg'] + 2],\n",
    "    'class_weights': [best_params['class_weights']],\n",
    "    'border_count': [best_params['border_count']],\n",
    "    'bagging_temperature': [best_params['bagging_temperature']]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=catboost_model,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  # 3-fold cross-validation\n",
    "                           scoring='roc_auc',  # Optimize for AUC\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)  # Use all processors\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best estimator after fitting\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict probabilities and classes for the test set using the best model\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Model Accuracy: {accuracy * 100:.5f}%\")\n",
    "print(f\"Model AUC: {auc * 100:.5f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found: \", grid_search.best_params_)"
   ],
   "id": "b7e03e5c1149e321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Model Accuracy: 84.10887%\n",
      "Model AUC: 82.30275%\n",
      "Confusion Matrix:\n",
      "[[843  74]\n",
      " [107 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90       917\n",
      "           1       0.61      0.52      0.56       222\n",
      "\n",
      "    accuracy                           0.84      1139\n",
      "   macro avg       0.75      0.72      0.73      1139\n",
      "weighted avg       0.83      0.84      0.84      1139\n",
      "\n",
      "Best hyperparameters found:  {'bagging_temperature': 0.5, 'border_count': 64, 'class_weights': [1, 2], 'depth': 4, 'iterations': 1000, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T07:35:37.302176Z",
     "start_time": "2024-11-11T07:35:36.631314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('extractedMimic.csv')\n",
    "for col in data.columns: # Check if the column is of type datetime by trying to convert \n",
    "    try: \n",
    "        data[col] = pd.to_datetime(data[col]) \n",
    "        # Convert datetime to timestamp difference from the first date in the column \n",
    "        data[col] = (data[col] - data[col].min()).dt.total_seconds() \n",
    "    except (ValueError, TypeError): # Column is not datetime; \n",
    "        # continue to the next step\n",
    "        continue # Step 2: Encode categorical variables with Label Encoding \n",
    "label_encoders = {} \n",
    "for col in data.select_dtypes(include=['object']).columns: # Apply label encoding for categorical columns \n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le # Save encoder for future reference # Step 3: Fill missing values # You can choose different strategies (e.g., mean, median, mode) depending on the data characteristics \n",
    "data = data.fillna(data.median())\n",
    "\n",
    "# Define features and target\n",
    "# Assuming `mortality` is the target column for third-day mortality\n",
    "X = data.drop(columns=['thirtyday_expire_flag'])\n",
    "y = data['thirtyday_expire_flag']\n",
    "\n",
    "# Preprocess data: handle missing values and standardize features\n",
    "X = X.fillna(X.median())  # Simple missing value imputation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 1. Filter Method: Univariate Selection with ANOVA F-test\n",
    "select_k_best = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
    "X_selected = select_k_best.fit_transform(X_scaled, y)\n",
    "selected_features_filter = X.columns[select_k_best.get_support()]\n",
    "\n",
    "print(\"Selected features using Filter Method (Univariate Selection):\")\n",
    "print(selected_features_filter)\n",
    "\n",
    "# 2. Wrapper Method: Recursive Feature Elimination (RFE) with RandomForestClassifier\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=20, step=1)\n",
    "X_selected_rfe = rfe.fit_transform(X_scaled, y)\n",
    "selected_features_rfe = X.columns[rfe.get_support()]\n",
    "\n",
    "print(\"Selected features using Wrapper Method (RFE):\")\n",
    "print(selected_features_rfe)\n",
    "\n",
    "# 3. Embedded Method: Lasso Regularization\n",
    "lasso = Lasso(alpha=0.01)  # Adjust alpha as needed\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_features_lasso = X.columns[(lasso.coef_ != 0)]\n",
    "\n",
    "print(\"Selected features using Embedded Method (Lasso):\")\n",
    "print(selected_features_lasso)\n",
    "\n",
    "# Combine selected features from each method for final model\n",
    "final_selected_features = list(set(selected_features_filter) | set(selected_features_rfe) | set(selected_features_lasso))\n",
    "X_final = X[final_selected_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a final model with selected features\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with selected features: {accuracy:.2f}\")"
   ],
   "id": "2eb6cd2690ec30f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\2818983608.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[col] = pd.to_datetime(data[col])\n",
      "C:\\Users\\lotus\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4 6] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\lotus\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using Filter Method (Univariate Selection):\n",
      "Index(['hospital_expire_flag', 'sepsis_explicit', 'severe_sepsis_explicit',\n",
      "       'elixhauser_hospital', 'sofa', 'lods', 'aniongap_min', 'aniongap_max',\n",
      "       'lactate_min', 'lactate_max', 'lactate_mean', 'bun_min', 'bun_max',\n",
      "       'bun_mean', 'sysbp_min', 'meanbp_min', 'resprate_mean', 'spo2_min',\n",
      "       'spo2_mean', 'urineoutput'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 48\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# 2. Wrapper Method: Recursive Feature Elimination (RFE) with RandomForestClassifier\u001B[39;00m\n\u001B[0;32m     47\u001B[0m rfe \u001B[38;5;241m=\u001B[39m RFE(estimator\u001B[38;5;241m=\u001B[39mRandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m), n_features_to_select\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 48\u001B[0m X_selected_rfe \u001B[38;5;241m=\u001B[39m rfe\u001B[38;5;241m.\u001B[39mfit_transform(X_scaled, y)\n\u001B[0;32m     49\u001B[0m selected_features_rfe \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mcolumns[rfe\u001B[38;5;241m.\u001B[39mget_support()]\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSelected features using Wrapper Method (RFE):\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_set_output.py:316\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 316\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    318\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    319\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    320\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    321\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    322\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1101\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m   1098\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m-> 1101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_rfe.py:268\u001B[0m, in \u001B[0;36mRFE.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \n\u001B[0;32m    250\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;124;03m    Fitted estimator.\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    267\u001B[0m _raise_for_unsupported_routing(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m--> 268\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_rfe.py:323\u001B[0m, in \u001B[0;36mRFE._fit\u001B[1;34m(self, X, y, step_score, **fit_params)\u001B[0m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting estimator with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m features.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m np\u001B[38;5;241m.\u001B[39msum(support_))\n\u001B[1;32m--> 323\u001B[0m estimator\u001B[38;5;241m.\u001B[39mfit(X[:, features], y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;66;03m# Get importance and rank them\u001B[39;00m\n\u001B[0;32m    326\u001B[0m importances \u001B[38;5;241m=\u001B[39m _get_feature_importances(\n\u001B[0;32m    327\u001B[0m     estimator,\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimportance_getter,\n\u001B[0;32m    329\u001B[0m     transform_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquare\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    330\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:421\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    414\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    415\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSum of y is not strictly positive which \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    416\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis necessary for Poisson regression.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    417\u001B[0m         )\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_samples, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m--> 421\u001B[0m y, expanded_class_weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_y_class_weight(y)\n\u001B[0;32m    423\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(y, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m!=\u001B[39m DOUBLE \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m y\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mcontiguous:\n\u001B[0;32m    424\u001B[0m     y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mascontiguousarray(y, dtype\u001B[38;5;241m=\u001B[39mDOUBLE)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:831\u001B[0m, in \u001B[0;36mForestClassifier._validate_y_class_weight\u001B[1;34m(self, y)\u001B[0m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_y_class_weight\u001B[39m(\u001B[38;5;28mself\u001B[39m, y):\n\u001B[1;32m--> 831\u001B[0m     check_classification_targets(y)\n\u001B[0;32m    833\u001B[0m     y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mcopy(y)\n\u001B[0;32m    834\u001B[0m     expanded_class_weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\multiclass.py:219\u001B[0m, in \u001B[0;36mcheck_classification_targets\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m    211\u001B[0m y_type \u001B[38;5;241m=\u001B[39m type_of_target(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel-sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    218\u001B[0m ]:\n\u001B[1;32m--> 219\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown label type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Maybe you are trying to fit a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassifier, which expects discrete classes on a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregression target with continuous values.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    223\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T07:36:02.366479Z",
     "start_time": "2024-11-11T07:36:02.244695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in data.columns: # Check if the column is of type datetime by trying to convert \n",
    "    try: \n",
    "        data[col] = pd.to_datetime(data[col]) \n",
    "        # Convert datetime to timestamp difference from the first date in the column \n",
    "        data[col] = (data[col] - data[col].min()).dt.total_seconds() \n",
    "    except (ValueError, TypeError): # Column is not datetime; \n",
    "        # continue to the next step\n",
    "        continue # Step 2: Encode categorical variables with Label Encoding \n",
    "label_encoders = {} \n",
    "for col in data.select_dtypes(include=['object']).columns: # Apply label encoding for categorical columns \n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le \n",
    "data = data.fillna(data.median())"
   ],
   "id": "afcde4115b097349",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T07:36:36.606715Z",
     "start_time": "2024-11-11T07:36:27.698511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define features and target\n",
    "# Assuming `mortality` is the target column for third-day mortality\n",
    "X = data.drop(columns=['thirtyday_expire_flag'])\n",
    "y = data['thirtyday_expire_flag']\n",
    "\n",
    "# Preprocess data: handle missing values and standardize features\n",
    "X = X.fillna(X.median())  # Simple missing value imputation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 1. Filter Method: Univariate Selection with ANOVA F-test\n",
    "select_k_best = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
    "X_selected = select_k_best.fit_transform(X_scaled, y)\n",
    "selected_features_filter = X.columns[select_k_best.get_support()]\n",
    "\n",
    "print(\"Selected features using Filter Method (Univariate Selection):\")\n",
    "print(selected_features_filter)\n",
    "\n",
    "# 2. Wrapper Method: Recursive Feature Elimination (RFE) with RandomForestClassifier\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=20, step=1)\n",
    "X_selected_rfe = rfe.fit_transform(X_scaled, y)\n",
    "selected_features_rfe = X.columns[rfe.get_support()]\n",
    "\n",
    "print(\"Selected features using Wrapper Method (RFE):\")\n",
    "print(selected_features_rfe)\n",
    "\n",
    "# 3. Embedded Method: Lasso Regularization\n",
    "lasso = Lasso(alpha=0.01)  # Adjust alpha as needed\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_features_lasso = X.columns[(lasso.coef_ != 0)]\n",
    "\n",
    "print(\"Selected features using Embedded Method (Lasso):\")\n",
    "print(selected_features_lasso)\n",
    "\n",
    "# Combine selected features from each method for final model\n",
    "final_selected_features = list(set(selected_features_filter) | set(selected_features_rfe) | set(selected_features_lasso))\n",
    "X_final = X[final_selected_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a final model with selected features\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with selected features: {accuracy:.2f}\")"
   ],
   "id": "ab8de5dbe9fd9417",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lotus\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  msb = ssbn / float(dfbn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using Filter Method (Univariate Selection):\n",
      "Index(['meanbp_mean', 'resprate_min', 'resprate_max', 'resprate_mean',\n",
      "       'tempc_min', 'tempc_max', 'tempc_mean', 'spo2_min', 'spo2_max',\n",
      "       'spo2_mean', 'glucose_min1', 'glucose_max1', 'glucose_mean', 'rrt',\n",
      "       'subject_id', 'hadm_id.1', 'icustay_id.1', 'urineoutput',\n",
      "       'colloid_bolus', 'crystalloid_bolus'],\n",
      "      dtype='object')\n",
      "Selected features using Wrapper Method (RFE):\n",
      "Index(['meanbp_mean', 'resprate_min', 'resprate_max', 'resprate_mean',\n",
      "       'tempc_min', 'tempc_max', 'tempc_mean', 'spo2_min', 'spo2_max',\n",
      "       'spo2_mean', 'glucose_min1', 'glucose_max1', 'glucose_mean', 'rrt',\n",
      "       'subject_id', 'hadm_id.1', 'icustay_id.1', 'urineoutput',\n",
      "       'colloid_bolus', 'crystalloid_bolus'],\n",
      "      dtype='object')\n",
      "Selected features using Embedded Method (Lasso):\n",
      "Index([], dtype='object')\n",
      "Model Accuracy with selected features: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lotus\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T07:45:25.430110Z",
     "start_time": "2024-11-11T07:45:25.257490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('extractedMimic.csv')\n",
    "\n",
    "# Select features and target\n",
    "X = df[['meanbp_mean', 'resprate_min', 'resprate_max', 'resprate_mean',\n",
    "       'tempc_min', 'tempc_max', 'tempc_mean', 'spo2_min', 'spo2_max',\n",
    "       'spo2_mean', 'glucose_min1', 'glucose_max1', 'glucose_mean', 'rrt',\n",
    "       'subject_id', 'hadm_id.1', 'icustay_id.1', 'urineoutput',\n",
    "       'colloid_bolus', 'crystalloid_bolus']]\n",
    "y = df['thirtyday_expire_flag']\n",
    "\n",
    "# Fill missing values with the median\n",
    "X.fillna(X.median(), inplace=True)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "# Train the XGBoost model with hyperparameter tuning\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss', \n",
    "    booster='gbtree', \n",
    "    objective='binary:logistic', \n",
    "    n_estimators=90,      # Number of trees\n",
    "    max_depth=6,          # Depth of trees\n",
    "    learning_rate=0.05,   # Learning rate\n",
    "    subsample=0.8,        # Percentage of data for each tree\n",
    "    colsample_bytree=0.8, # Feature subsampling\n",
    "    gamma=0.05            # Regularization parameter\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)  # Predicted labels\n",
    "y_probs = xgb_model.predict_proba(X_test)[:, 1]  # Predicted probabilities for class 1\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_probs)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Model Accuracy: {accuracy * 100:.5f}%\")\n",
    "print(f\"Model AUC: {auc * 100:.5f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ],
   "id": "c2e20f3f8d51c5a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3647, 20)\n",
      "Testing set shape: (912, 20)\n",
      "Model Accuracy: 84.32018%\n",
      "Model AUC: 80.50519%\n",
      "Confusion Matrix:\n",
      "[[721  19]\n",
      " [124  48]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91       740\n",
      "           1       0.72      0.28      0.40       172\n",
      "\n",
      "    accuracy                           0.84       912\n",
      "   macro avg       0.78      0.63      0.66       912\n",
      "weighted avg       0.83      0.84      0.81       912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lotus\\AppData\\Local\\Temp\\ipykernel_11784\\597179868.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.fillna(X.median(), inplace=True)\n",
      "C:\\Users\\lotus\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [11:15:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae0d3b714f815d7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
